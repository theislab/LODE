{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "import pandas as pd\n",
    "from sklearn import datasets\n",
    "import seaborn as sns\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import GroupKFold\n",
    "import datetime\n",
    "import seaborn as sns\n",
    "\n",
    "WORK_SPACE = \"/home/olle/PycharmProjects/LODE/workspace\"\n",
    "\n",
    "longitudinal_pd = pd.read_csv(os.path.join(WORK_SPACE, \"sequence_data/longitudinal_data.csv\"))\n",
    "segmentation_statistics_pd = pd.read_csv(os.path.join(WORK_SPACE, \"sequence_data/segmentation_statistics_vol.csv\"))\n",
    "\n",
    "id_cols = segmentation_statistics_pd.record.str.split(\"_\", expand=True)[[0, 1 , 2, 3]]\n",
    "\n",
    "segmentation_statistics_pd[\"patient_id\"] = id_cols[0]\n",
    "segmentation_statistics_pd[\"study_date\"] = id_cols[1]\n",
    "segmentation_statistics_pd[\"laterality\"] = id_cols[2]\n",
    "\n",
    "longitudinal_pd[\"study_date\"] = longitudinal_pd.study_date.str.replace(\"-\", \"\")\n",
    "\n",
    "# cast data types\n",
    "keys = [\"patient_id\", \"study_date\", \"laterality\"] \n",
    "for key in keys:\n",
    "    longitudinal_pd[key] = longitudinal_pd[key].astype(str)\n",
    "    segmentation_statistics_pd[key] = segmentation_statistics_pd[key].astype(str)\n",
    "    \n",
    "longitudinal_abt = pd.merge(longitudinal_pd, segmentation_statistics_pd, how=\"inner\", \n",
    "                            left_on = keys, right_on = keys)\n",
    "\n",
    "feature_columns = list(longitudinal_abt.columns[17:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### split data based on group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((18549, 126), (4411, 126))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "X = longitudinal_abt[feature_columns]\n",
    "y = longitudinal_abt.logMAR\n",
    "groups = longitudinal_abt.patient_id\n",
    "\n",
    "gss = GroupShuffleSplit(n_splits=1, train_size=.8, random_state=42)\n",
    "\n",
    "train_idx, eval_idx = next(gss.split(X, y, groups))\n",
    "\n",
    "train_x = X.iloc[train_idx]\n",
    "train_y = y.iloc[train_idx]\n",
    "\n",
    "test_x = X.iloc[eval_idx]\n",
    "test_y = y.iloc[eval_idx]\n",
    "\n",
    "train_x.shape, test_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         34537\n",
       "1         34537\n",
       "2         34537\n",
       "3         34537\n",
       "4         34537\n",
       "          ...  \n",
       "22955    121331\n",
       "22956    121685\n",
       "22957    122659\n",
       "22958    126011\n",
       "22959    126011\n",
       "Name: patient_id, Length: 22960, dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ScaledLR: 0.362298 (0.043424)\n",
      "ScaledLASSO: -0.004347 (0.003840)\n",
      "ScaledEN: -0.004347 (0.003840)\n",
      "ScaledKNN: 0.250898 (0.049855)\n",
      "ScaledCART: -0.117768 (0.124853)\n",
      "ScaledGBM: 0.444334 (0.029202)\n"
     ]
    }
   ],
   "source": [
    "pipelines = []\n",
    "pipelines.append(('ScaledLR', Pipeline([('Scaler', StandardScaler()),('LR',LinearRegression())])))\n",
    "pipelines.append(('ScaledLASSO', Pipeline([('Scaler', StandardScaler()),('LASSO', Lasso())])))\n",
    "pipelines.append(('ScaledEN', Pipeline([('Scaler', StandardScaler()),('EN', ElasticNet())])))\n",
    "pipelines.append(('ScaledKNN', Pipeline([('Scaler', StandardScaler()),('KNN', KNeighborsRegressor())])))\n",
    "pipelines.append(('ScaledCART', Pipeline([('Scaler', StandardScaler()),('CART', DecisionTreeRegressor())])))\n",
    "pipelines.append(('ScaledGBM', Pipeline([('Scaler', StandardScaler()),('GBM', GradientBoostingRegressor())])))\n",
    "\n",
    "results = []\n",
    "names = []\n",
    "for name, model in pipelines:\n",
    "    kfoldGroup = GroupKFold(n_splits=5)\n",
    "    cv_results = cross_val_score(model, X, y, groups, cv=kfoldGroup, scoring='r2')\n",
    "    results.append(cv_results)\n",
    "    names.append(name)\n",
    "    msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n",
    "    print(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### perform grid search over GradientBooster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.449095 (0.044104) with: {'n_estimators': 50}\n",
      "0.454284 (0.045447) with: {'n_estimators': 100}\n",
      "0.451467 (0.046835) with: {'n_estimators': 200}\n",
      "0.447170 (0.047009) with: {'n_estimators': 300}\n",
      "0.443561 (0.047088) with: {'n_estimators': 400}\n",
      "Best: 0.454284 using {'n_estimators': 100}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "scaler = StandardScaler().fit(train_x)\n",
    "rescaledX = scaler.transform(train_x)\n",
    "param_grid = dict(n_estimators=np.array([50,100,200,300,400]))\n",
    "model = GradientBoostingRegressor(random_state=21)\n",
    "kfold = KFold(n_splits=5)\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, scoring='r2', cv=kfold)\n",
    "grid_result = grid.fit(rescaledX, train_y)\n",
    "\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))\n",
    "\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### finalize model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2454949512099583\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "scaler = StandardScaler().fit(train_x)\n",
    "rescaled_X_train = scaler.transform(train_x)\n",
    "model = GradientBoostingRegressor(random_state=21, n_estimators=100)\n",
    "model.fit(rescaled_X_train, train_y)\n",
    "\n",
    "# transform the validation dataset\n",
    "rescaled_X_test = scaler.transform(test_x)\n",
    "predictions = model.predict(rescaled_X_test)\n",
    "print(mean_absolute_error(test_y, predictions))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "visualization",
   "language": "python",
   "name": "visualization"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
