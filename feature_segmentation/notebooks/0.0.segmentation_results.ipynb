{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs, json, glob, os, sys\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import confusion_matrix, jaccard_score, classification_report\n",
    "import numpy as np\n",
    "from sklearn.metrics import f1_score\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from utils.utils import label_mapping\n",
    "from utils.image_processing import read_resize\n",
    "from config import TRAIN_DATA_PATH\n",
    "from utils.plotting import plot_image_label_prediction, plot_uncertainty_heatmaps, \\\n",
    "    plot_uncertainty_statistics, plot_image, plot_label, plot_predictions, plot_idv_records\n",
    "\n",
    "result_json_dir = \"/home/olle/PycharmProjects/LODE/workspace/feature_segmentation/opt_ensemble\"\n",
    "\n",
    "configured_labels = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 13, 14, 15]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load json files to evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_decode_json(file_path):\n",
    "    obj_text = codecs.open(file_path, 'r', encoding='utf-8').read()\n",
    "    \n",
    "    json_result_file = json.loads(obj_text)\n",
    "\n",
    "    json_result_file[\"image\"] = np.array(json_result_file[\"image\"])\n",
    "    json_result_file[\"label\"] = np.array(json_result_file[\"label\"])\n",
    "    json_result_file[\"prediction\"] = np.array(json_result_file[\"prediction\"])\n",
    "    return json_result_file\n",
    "    \n",
    "json_result_paths = glob.glob(result_json_dir + \"/*.json\")\n",
    "\n",
    "result_jsons = {}\n",
    "for json_file_path in json_result_paths:\n",
    "    record = json_file_path.split(\"/\")[-1].replace(\".json\", \"\")\n",
    "    result_jsons[record] = read_decode_json(json_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## plot records for publication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "records = result_jsons.keys()\n",
    "\n",
    "for record in records:\n",
    "    record_log = result_jsons[record]\n",
    "    data = [record_log[\"image\"], record_log[\"label\"], record_log[\"prediction\"]]\n",
    "    \n",
    "    # all plotting functions\n",
    "    plot_image_label_prediction(data,model_dir = result_json_dir, filename=record, mode=\"test\")\n",
    "    plot_image(record_log[\"image\"], model_dir = result_json_dir, filename=record, mode=\"test\")\n",
    "    plot_label(record_log[\"label\"], model_dir = result_json_dir, filename=record, mode=\"test\")\n",
    "    plot_predictions([record_log[\"prediction\"]], model_dir = result_json_dir, filename=record, mode=\"test\")\n",
    "    \n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## create results per record result logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "records = result_jsons.keys()\n",
    "\n",
    "f1_record_log = {}\n",
    "f1_total_log = {}\n",
    "\n",
    "all_labels = []\n",
    "all_predictions = []\n",
    "\n",
    "for record in records:\n",
    "    record_log = result_jsons[record]\n",
    "    \n",
    "    labels = record_log[\"label\"].flatten()\n",
    "    predictions = record_log[\"prediction\"].flatten()\n",
    "    \n",
    "    all_labels.extend(labels.tolist())\n",
    "    all_predictions.extend(predictions.tolist())\n",
    "    \n",
    "    f1_record_log[record] = f1_score(labels, \n",
    "                         predictions, \n",
    "                         average = None, \n",
    "                         labels = configured_labels, \n",
    "                         zero_division=1)\n",
    "\n",
    "# calculate for each log\n",
    "f1_total_log = f1_score(all_labels, \n",
    "                        all_predictions, \n",
    "                        average = None, \n",
    "                        labels = configured_labels, \n",
    "                        zero_division=1)\n",
    "\n",
    "# convert to data frames for plotting\n",
    "total_f1_pd = pd.DataFrame(f1_total_log).T.replace(1.0,np.NaN)\n",
    "record_f1_pd = pd.DataFrame(f1_record_log).T.replace(1.0,np.NaN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_f1_pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inter doctor variation analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set env variables\n",
    "idv_json_dir = \"/home/olle/PycharmProjects/LODE/workspace/feature_segmentation/opt_ensemble/idv\"\n",
    "\n",
    "evaluators = [\"ben\", \"johannes\", \"michael\", \"concensus\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_decode_json(file_path):\n",
    "    obj_text = codecs.open(file_path, 'r', encoding='utf-8').read()\n",
    "    \n",
    "    json_result_file = json.loads(obj_text)\n",
    "    json_result_file[\"image\"] = np.array(json_result_file[\"image\"])\n",
    "    \n",
    "    # add all evaluators results\n",
    "    for evaluator in evaluators:\n",
    "        json_result_file[f\"{evaluator}_mask\"] = np.array(json_result_file[f\"{evaluator}_mask\"])\n",
    "\n",
    "    json_result_file[\"prediction\"] = np.array(json_result_file[\"prediction\"])\n",
    "    return json_result_file\n",
    "    \n",
    "json_result_paths = glob.glob(idv_json_dir + \"/*.json\")\n",
    "\n",
    "result_jsons = {}\n",
    "for json_file_path in json_result_paths:\n",
    "    record = json_file_path.split(\"/\")[-1].replace(\".json\", \"\")\n",
    "    result_jsons[record] = read_decode_json(json_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## plot all records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/olle/PycharmProjects/LODE/workspace/feature_segmentation/opt_ensemble/idv'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idv_json_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "records = result_jsons.keys()\n",
    "\n",
    "for record in records:\n",
    "    record_log = result_jsons[record]\n",
    "    data = [record_log[\"image\"], record_log[\"concensus_mask\"], record_log[\"prediction\"]]\n",
    "    \n",
    "    idv_data = [record_log[\"image\"], record_log[\"concensus_mask\"], \n",
    "                record_log[\"ben_mask\"], record_log[\"johannes_mask\"], \n",
    "                record_log[\"michael_mask\"], record_log[\"prediction\"]]\n",
    "\n",
    "    # all plotting functions\n",
    "    plot_image_label_prediction(data,model_dir = idv_json_dir, filename=record, mode=\"test\")\n",
    "    plot_image(record_log[\"image\"], model_dir = idv_json_dir, filename=record, mode=\"test\")\n",
    "    \n",
    "    plot_idv_records(idv_data, model_dir = idv_json_dir, filename=record, mode=\"idv_test\")\n",
    "     # add all evaluators results\n",
    "    for evaluator in evaluators:\n",
    "        plot_label(record_log[f\"{evaluator}_mask\"], \n",
    "                   model_dir = idv_json_dir, \n",
    "                   filename=f\"{evaluator}_{record}\", \n",
    "                   mode=\"test\")\n",
    "\n",
    "    plot_predictions([record_log[\"prediction\"]], model_dir = idv_json_dir, filename=record, mode=\"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## extract results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "records = result_jsons.keys()\n",
    "\n",
    "f1_record_log = {}\n",
    "f1_total_log = {}\n",
    "\n",
    "labels_predictions = {\"concensus\": []}\n",
    "\n",
    "for evaluator in evaluators + [\"prediction\"]:\n",
    "    labels_predictions[evaluator] = []\n",
    "        \n",
    "for record in records:\n",
    "    record_log = result_jsons[record]\n",
    "    \n",
    "    labels_predictions[\"prediction\"].extend(record_log[f\"prediction\"].flatten().tolist())\n",
    "\n",
    "    for evaluator in evaluators:\n",
    "        labels_predictions[evaluator].extend(record_log[f\"{evaluator}_mask\"].flatten().tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idv_logs = {}\n",
    "for evaluator in evaluators + [\"prediction\"]:\n",
    "    # calculate for each log\n",
    "    f1_total_log = f1_score(labels_predictions[\"concensus\"], \n",
    "                            labels_predictions[evaluator], \n",
    "                            average = None, \n",
    "                            labels = configured_labels, \n",
    "                            zero_division=1)\n",
    "\n",
    "    idv_logs[evaluator] = f1_total_log\n",
    "    \n",
    "# convert to data frames for plotting\n",
    "total_f1_pd = pd.DataFrame(idv_logs).T.replace(1.0,np.NaN)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(idv_logs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "total_f1_pd"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "segmentation",
   "language": "python",
   "name": "segmentation"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
