{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs, json, glob, os, sys\n",
    "\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import confusion_matrix, jaccard_score, classification_report, f1_score\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "from generator import label_mapping\n",
    "from utils.image_processing import read_resize\n",
    "\n",
    "from utils.plotting import plot_image_label_prediction, plot_uncertainty_heatmaps, \\\n",
    "    plot_uncertainty_statistics, plot_image, plot_label, plot_predictions\n",
    "\n",
    "VALIDATION_RESULT_DIR = \"/home/olle/PycharmProjects/LODE/workspace/feature_segmentation/cv_runs_results\"\n",
    "\n",
    "models = os.listdir(VALIDATION_RESULT_DIR)\n",
    "\n",
    "configured_labels = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 13, 14, 15]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load json files to evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_decode_json(file_path):\n",
    "    obj_text = codecs.open(file_path, 'r', encoding='utf-8').read()\n",
    "    \n",
    "    json_result_file = json.loads(obj_text)\n",
    "    \n",
    "    image = np.array(json_result_file[\"image\"])\n",
    "    prediction = np.array(json_result_file[\"prediction\"])\n",
    "    \n",
    "    if len(image.shape) > 3:\n",
    "        image = image[0, :, :, :]\n",
    "        \n",
    "    if len(prediction.shape) > 2:\n",
    "        prediction = prediction[0, :, :]\n",
    "    \n",
    "    json_result_file[\"image\"] = image\n",
    "    json_result_file[\"label\"] = np.array(json_result_file[\"label\"])\n",
    "    json_result_file[\"prediction\"] = prediction\n",
    "    return json_result_file\n",
    "\n",
    "def read_results(model):\n",
    "    # set paths\n",
    "    model_dir = os.path.join(VALIDATION_RESULT_DIR, model)\n",
    "    json_result_paths = glob.glob(model_dir + f\"/*.json\")\n",
    "\n",
    "    result_jsons = {}\n",
    "    \n",
    "    for json_file_path in json_result_paths:\n",
    "        record = json_file_path.split(\"/\")[-1].replace(\".json\", \"\")\n",
    "        \n",
    "        if \"model\" in record:\n",
    "            continue\n",
    "            \n",
    "        result_jsons[record] = read_decode_json(json_file_path)\n",
    "    return result_jsons\n",
    "        \n",
    "def plot_results(result_jsons, model):\n",
    "    records = result_jsons.keys()\n",
    "    \n",
    "    model_dir = os.path.join(VALIDATION_RESULT_DIR, model)\n",
    "\n",
    "    for record in records:\n",
    "        record_log = result_jsons[record]\n",
    "        data = [record_log[\"image\"], record_log[\"label\"], record_log[\"prediction\"]]\n",
    "\n",
    "        # all plotting functions\n",
    "        plot_image_label_prediction(data, model_dir = model_dir, filename=record, mode=\"test\")\n",
    "        plot_image(record_log[\"image\"], model_dir = model_dir, filename=record, mode=\"test\")\n",
    "        plot_label(record_log[\"label\"], model_dir = model_dir, filename=record, mode=\"test\")\n",
    "        plot_predictions([record_log[\"prediction\"]], model_dir = model_dir, filename=record, mode=\"test\")\n",
    "        \n",
    "        \n",
    "def get_results(result_jsons):\n",
    "    records = result_jsons.keys()\n",
    "\n",
    "    f1_record_log = {}\n",
    "    f1_total_log = {}\n",
    "\n",
    "    all_labels = []\n",
    "    all_predictions = []\n",
    "\n",
    "    for record in records:\n",
    "        record_log = result_jsons[record]\n",
    "\n",
    "        labels = record_log[\"label\"].flatten()\n",
    "        predictions = record_log[\"prediction\"].flatten()\n",
    "\n",
    "        all_labels.extend(labels.tolist())\n",
    "        all_predictions.extend(predictions.tolist())\n",
    "\n",
    "        f1_record_log[record] = f1_score(labels, \n",
    "                             predictions, \n",
    "                             average = None, \n",
    "                             labels = configured_labels, \n",
    "                             zero_division=1)\n",
    "\n",
    "    # calculate for each log\n",
    "    f1_total_log = f1_score(all_labels, \n",
    "                            all_predictions, \n",
    "                            average = None, \n",
    "                            labels = configured_labels, \n",
    "                            zero_division=1)\n",
    "\n",
    "    # convert to data frames for plotting\n",
    "    total_f1_pd = pd.DataFrame(f1_total_log).T.replace(1.0, np.NaN)\n",
    "    record_f1_pd = pd.DataFrame(f1_record_log).T.replace(1.0, np.NaN)\n",
    "    \n",
    "    return total_f1_pd, record_f1_pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = np.array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13])\n",
    "validation_log = pd.DataFrame(columns=columns)\n",
    "\n",
    "from concurrent.futures import ProcessPoolExecutor, ThreadPoolExecutor\n",
    "import time\n",
    "\n",
    "def fill_validation_log(model):\n",
    "    jsons = read_results(model)\n",
    "    plot_results(jsons, model)\n",
    "    total_r, record_r = get_results(jsons)\n",
    "    return total_r\n",
    "    # validation_log = validation_log.append(total_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=2)]: Using backend MultiprocessingBackend with 2 concurrent workers.\n"
     ]
    }
   ],
   "source": [
    "from joblib import Parallel\n",
    "\n",
    "results = Parallel(n_jobs=2, verbose=1, backend=\"multiprocessing\")(map(delayed(fill_validation_log), \n",
    "                                                                       models))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = np.array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13])\n",
    "validation_log = pd.DataFrame(columns=columns)\n",
    "\n",
    "for i in range(len(results)):\n",
    "    validation_log = validation_log.append(pd.DataFrame(results[i]))\n",
    "    \n",
    "validation_log[\"model\"] = models[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_log_long = pd.melt(validation_log, id_vars=\"model\")\n",
    "validation_log_long = validation_log_long.rename(columns={\"variable\": \"feature\", \"value\":\"f1-score\"})\n",
    "\n",
    "validation_log_long = validation_log_long[~validation_log_long.feature.isin([0, 2, 12, 13])]\n",
    "import seaborn as sns\n",
    "sns.set(rc={'figure.figsize':(15,7)})\n",
    "sns.set_context(\"paper\", rc={\"font.size\":20,\"axes.titlesize\":20,\"axes.labelsize\":20})   \n",
    "\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "ax = sns.barplot(x=\"feature\", y=\"f1-score\", hue=\"model\", data=validation_log_long)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tracking_library",
   "language": "python",
   "name": "tracking_library"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
