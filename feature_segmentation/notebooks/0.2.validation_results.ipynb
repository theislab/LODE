{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs, json, glob, os, sys\n",
    "\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import confusion_matrix, jaccard_score, classification_report, f1_score\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from generator import label_mapping\n",
    "from utils.image_processing import read_resize\n",
    "\n",
    "from utils.plotting import plot_image_label_prediction, plot_uncertainty_heatmaps, \\\n",
    "    plot_uncertainty_statistics, plot_image, plot_label, plot_predictions\n",
    "\n",
    "validation_result_dir = \"/home/olle/PycharmProjects/LODE/workspace/validation_model_selection\"\n",
    "\n",
    "models = [\"135590_L_20171201_586379001_17\"]\n",
    "\n",
    "configured_labels = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 13, 14, 15]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load json files to evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_decode_json(file_path):\n",
    "    obj_text = codecs.open(file_path, 'r', encoding='utf-8').read()\n",
    "    \n",
    "    json_result_file = json.loads(obj_text)\n",
    "    \n",
    "    image = np.array(json_result_file[\"image\"])\n",
    "    prediction = np.array(json_result_file[\"prediction\"])\n",
    "    \n",
    "    if len(image.shape) > 3:\n",
    "        image = image[0, :, :, :]\n",
    "        \n",
    "    if len(prediction.shape) > 2:\n",
    "        prediction = prediction[0, :, :]\n",
    "    \n",
    "\n",
    "    json_result_file[\"image\"] = image\n",
    "    json_result_file[\"label\"] = np.array(json_result_file[\"label\"])\n",
    "    json_result_file[\"prediction\"] = prediction\n",
    "    return json_result_file\n",
    "\n",
    "# specify model to be evaluated\n",
    "model = models[0]\n",
    "\n",
    "# set paths\n",
    "model_dir = os.path.join(validation_result_dir, model)\n",
    "json_result_paths = glob.glob(model_dir + f\"/validation_results/*.json\")\n",
    "\n",
    "result_jsons = {}\n",
    "\n",
    "for json_file_path in json_result_paths:\n",
    "    record = json_file_path.split(\"/\")[-1].replace(\".json\", \"\")\n",
    "    result_jsons[record] = read_decode_json(json_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## plot records for publication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "records = result_jsons.keys()\n",
    "\n",
    "for record in records:\n",
    "    record_log = result_jsons[record]\n",
    "    data = [record_log[\"image\"], record_log[\"label\"], record_log[\"prediction\"]]\n",
    "    \n",
    "    # all plotting functions\n",
    "    plot_image_label_prediction(data, model_dir = model_dir, filename=record, mode=\"test\")\n",
    "    plot_image(record_log[\"image\"], model_dir = model_dir, filename=record, mode=\"test\")\n",
    "    plot_label(record_log[\"label\"], model_dir = model_dir, filename=record, mode=\"test\")\n",
    "    plot_predictions([record_log[\"prediction\"]], model_dir = model_dir, filename=record, mode=\"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## create results per record result logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "records = result_jsons.keys()\n",
    "\n",
    "f1_record_log = {}\n",
    "f1_total_log = {}\n",
    "\n",
    "all_labels = []\n",
    "all_predictions = []\n",
    "\n",
    "for record in records:\n",
    "    record_log = result_jsons[record]\n",
    "    \n",
    "    labels = record_log[\"label\"].flatten()\n",
    "    predictions = record_log[\"prediction\"].flatten()\n",
    "    \n",
    "    all_labels.extend(labels.tolist())\n",
    "    all_predictions.extend(predictions.tolist())\n",
    "    \n",
    "    f1_record_log[record] = f1_score(labels, \n",
    "                         predictions, \n",
    "                         average = None, \n",
    "                         labels = configured_labels, \n",
    "                         zero_division=1)\n",
    "\n",
    "# calculate for each log\n",
    "f1_total_log = f1_score(all_labels, \n",
    "                        all_predictions, \n",
    "                        average = None, \n",
    "                        labels = configured_labels, \n",
    "                        zero_division=1)\n",
    "\n",
    "# convert to data frames for plotting\n",
    "total_f1_pd = pd.DataFrame(f1_total_log).T.replace(1.0,np.NaN)\n",
    "record_f1_pd = pd.DataFrame(f1_record_log).T.replace(1.0,np.NaN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.976742</td>\n",
       "      <td>0.650451</td>\n",
       "      <td>0.974769</td>\n",
       "      <td>0.858779</td>\n",
       "      <td>0.969274</td>\n",
       "      <td>0.250573</td>\n",
       "      <td>0.808349</td>\n",
       "      <td>0.729999</td>\n",
       "      <td>0.258522</td>\n",
       "      <td>0.742337</td>\n",
       "      <td>0.843468</td>\n",
       "      <td>0.735982</td>\n",
       "      <td>0.993516</td>\n",
       "      <td>0.983169</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         0         1         2         3         4         5         6   \\\n",
       "0  0.976742  0.650451  0.974769  0.858779  0.969274  0.250573  0.808349   \n",
       "\n",
       "         7         8         9         10        11        12        13  \n",
       "0  0.729999  0.258522  0.742337  0.843468  0.735982  0.993516  0.983169  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_f1_pd"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tracking_library",
   "language": "python",
   "name": "tracking_library"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
