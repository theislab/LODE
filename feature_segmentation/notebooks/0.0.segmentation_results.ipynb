{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs, json, glob, os, sys\n",
    "\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import confusion_matrix, jaccard_score, classification_report, f1_score\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from generator import label_mapping\n",
    "from utils.image_processing import read_resize\n",
    "\n",
    "from utils.plotting import plot_image_label_prediction, plot_uncertainty_heatmaps, \\\n",
    "    plot_uncertainty_statistics, plot_image, plot_label, plot_predictions, plot_idv_records\n",
    "\n",
    "result_json_dir = \"/home/olle/PycharmProjects/LODE/workspace/feature_segmentation/opt_ensemble2\"\n",
    "\n",
    "configured_labels = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 13, 14, 15]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load json files to evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_decode_json(file_path):\n",
    "    obj_text = codecs.open(file_path, 'r', encoding='utf-8').read()\n",
    "    \n",
    "    json_result_file = json.loads(obj_text)\n",
    "\n",
    "    json_result_file[\"image\"] = np.array(json_result_file[\"image\"])\n",
    "    json_result_file[\"label\"] = np.array(json_result_file[\"label\"])\n",
    "    json_result_file[\"prediction\"] = np.array(json_result_file[\"prediction\"])\n",
    "    return json_result_file\n",
    "    \n",
    "json_result_paths = glob.glob(result_json_dir + \"/*.json\")\n",
    "\n",
    "result_jsons = {}\n",
    "for json_file_path in json_result_paths:\n",
    "    record = json_file_path.split(\"/\")[-1].replace(\".json\", \"\")\n",
    "    result_jsons[record] = read_decode_json(json_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## plot records for publication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "records = result_jsons.keys()\n",
    "\n",
    "for record in records:\n",
    "    record_log = result_jsons[record]\n",
    "    data = [record_log[\"image\"], record_log[\"label\"], record_log[\"prediction\"]]\n",
    "    \n",
    "    # all plotting functions\n",
    "    plot_image_label_prediction(data,model_dir = result_json_dir, filename=record, mode=\"test\")\n",
    "    plot_image(record_log[\"image\"], model_dir = result_json_dir, filename=record, mode=\"test\")\n",
    "    plot_label(record_log[\"label\"], model_dir = result_json_dir, filename=record, mode=\"test\")\n",
    "    plot_predictions([record_log[\"prediction\"]], model_dir = result_json_dir, filename=record, mode=\"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## create results per record result logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "records = result_jsons.keys()\n",
    "\n",
    "f1_record_log = {}\n",
    "f1_total_log = {}\n",
    "\n",
    "all_labels = []\n",
    "all_predictions = []\n",
    "\n",
    "for record in records:\n",
    "    record_log = result_jsons[record]\n",
    "    \n",
    "    labels = record_log[\"label\"].flatten()\n",
    "    predictions = record_log[\"prediction\"].flatten()\n",
    "    \n",
    "    all_labels.extend(labels.tolist())\n",
    "    all_predictions.extend(predictions.tolist())\n",
    "    \n",
    "    f1_record_log[record] = f1_score(labels, \n",
    "                         predictions, \n",
    "                         average = None, \n",
    "                         labels = configured_labels, \n",
    "                         zero_division=1)\n",
    "\n",
    "# calculate for each log\n",
    "f1_total_log = f1_score(all_labels, \n",
    "                        all_predictions, \n",
    "                        average = None, \n",
    "                        labels = configured_labels, \n",
    "                        zero_division=1)\n",
    "\n",
    "# convert to data frames for plotting\n",
    "total_f1_pd = pd.DataFrame(f1_total_log).T.replace(1.0,np.NaN)\n",
    "record_f1_pd = pd.DataFrame(f1_record_log).T.replace(1.0,np.NaN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.977930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.540307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.974669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.749095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.973835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.523834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.823458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.734856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.628045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.595807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.835061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.800038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.993246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.978370</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           0\n",
       "0   0.977930\n",
       "1   0.540307\n",
       "2   0.974669\n",
       "3   0.749095\n",
       "4   0.973835\n",
       "5   0.523834\n",
       "6   0.823458\n",
       "7   0.734856\n",
       "8   0.628045\n",
       "9   0.595807\n",
       "10  0.835061\n",
       "11  0.800038\n",
       "12  0.993246\n",
       "13  0.978370"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_f1_pd.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inter doctor variation analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set env variables\n",
    "idv_json_dir = \"/home/olle/PycharmProjects/LODE/workspace/feature_segmentation/opt_ensemble2/idv\"\n",
    "evaluators = [\"ben\", \"johannes\", \"michael\", \"concensus\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_decode_json(file_path):\n",
    "    obj_text = codecs.open(file_path, 'r', encoding='utf-8').read()\n",
    "    \n",
    "    json_result_file = json.loads(obj_text)\n",
    "    json_result_file[\"image\"] = np.array(json_result_file[\"image\"])\n",
    "    \n",
    "    # add all evaluators results\n",
    "    for evaluator in evaluators:\n",
    "        json_result_file[f\"{evaluator}_mask\"] = np.array(json_result_file[f\"{evaluator}_mask\"])\n",
    "\n",
    "    json_result_file[\"prediction\"] = np.array(json_result_file[\"prediction\"])\n",
    "    return json_result_file\n",
    "    \n",
    "json_result_paths = glob.glob(idv_json_dir + \"/*.json\")\n",
    "\n",
    "result_jsons = {}\n",
    "for json_file_path in json_result_paths:\n",
    "    record = json_file_path.split(\"/\")[-1].replace(\".json\", \"\")\n",
    "    result_jsons[record] = read_decode_json(json_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## plot all records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "records = result_jsons.keys()\n",
    "\n",
    "for record in records:\n",
    "    record_log = result_jsons[record]\n",
    "    data = [record_log[\"image\"], record_log[\"concensus_mask\"], record_log[\"prediction\"]]\n",
    "    \n",
    "    # all plotting functions\n",
    "    plot_image_label_prediction(data,model_dir = idv_json_dir, filename=record, mode=\"test\")\n",
    "    plot_image(record_log[\"image\"], model_dir = idv_json_dir, filename=record, mode=\"test\")\n",
    "    \n",
    "    records_idv = []\n",
    "    records_idv.append(record_log[\"image\"])\n",
    "    records_idv.append(record_log[\"concensus_mask\"])\n",
    "    \n",
    "     # add all evaluators results\n",
    "    for evaluator in evaluators:\n",
    "        plot_label(record_log[f\"{evaluator}_mask\"], \n",
    "                   model_dir = idv_json_dir, \n",
    "                   filename=f\"{evaluator}_{record}\", \n",
    "                   mode=\"test\")\n",
    "        \n",
    "    \n",
    "    for name in [\"ben\", \"johannes\", \"michael\"]:\n",
    "        records_idv.append(record_log[f\"{name}_mask\"])\n",
    "\n",
    "    \n",
    "    records_idv.append(record_log[\"prediction\"])\n",
    "\n",
    "    plot_predictions([record_log[\"prediction\"]], model_dir = idv_json_dir, filename=record, mode=\"test\")\n",
    "    \n",
    "    \n",
    "    plot_idv_records(records_idv, model_dir=idv_json_dir, mode=\"test_idv\", filename=record)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## extract results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "records = result_jsons.keys()\n",
    "\n",
    "f1_record_log = {}\n",
    "f1_total_log = {}\n",
    "\n",
    "labels_predictions = {\"concensus\": []}\n",
    "\n",
    "for evaluator in evaluators + [\"prediction\"]:\n",
    "    labels_predictions[evaluator] = []\n",
    "        \n",
    "for record in records:\n",
    "    record_log = result_jsons[record]\n",
    "    \n",
    "    labels_predictions[\"prediction\"].extend(record_log[f\"prediction\"].flatten().tolist())\n",
    "\n",
    "    for evaluator in evaluators:\n",
    "        labels_predictions[evaluator].extend(record_log[f\"{evaluator}_mask\"].flatten().tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "idv_logs = {}\n",
    "for evaluator in evaluators + [\"prediction\"]:\n",
    "    # calculate for each log\n",
    "    f1_total_log = f1_score(labels_predictions[\"concensus\"], \n",
    "                            labels_predictions[evaluator], \n",
    "                            average = None, \n",
    "                            labels = configured_labels, \n",
    "                            zero_division=1)\n",
    "\n",
    "    idv_logs[evaluator] = f1_total_log\n",
    "    \n",
    "# convert to data frames for plotting\n",
    "total_f1_pd = pd.DataFrame(idv_logs).T.replace(1.0,np.NaN)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "idv_pd = pd.DataFrame(idv_logs)\n",
    "idv_pd = idv_pd.reset_index()\n",
    "\n",
    "idv_pd_long = pd.melt(idv_pd, id_vars=\"index\")\n",
    "\n",
    "idv_pd_long = idv_pd_long[~idv_pd_long.variable.isin([\"concensus\"])]\n",
    "\n",
    "idv_pd_long = idv_pd_long.rename(columns={\"variable\":\"annotator\", \"value\":\"f1-score\", \"index\":\"class\"})\n",
    "\n",
    "idv_pd_long = idv_pd_long[idv_pd_long[\"class\"].isin([\"5\", \"7\", \"8\", \"11\"])]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "idv_pd_long.loc[:, \"annotator\"] = idv_pd_long.annotator.replace({\"ben\": \"annotator I\", \n",
    "                                                                 \"johannes\": \"annotator II\", \n",
    "                                                                 \"michael\": \"annotator III\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sns' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-4baa6f02140c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m               10:\"choroid\", 11:\"fibrosis\", 14:\"vitreous\", 15:\"camera effect\"}\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0msns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_palette\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Greys\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0max\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbarplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"class\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"f1-score\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"annotator\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0midv_pd_long\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m plt.xticks(ticks=[0,1,2,3], labels=[class_dict[5], class_dict[7], class_dict[8], class_dict[11]],\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sns' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "class_dict = {0:\"background\", 1:\"epiretinal membrane\", 2:\"neurosensory retina\", \n",
    "              3:\"intraretinal fluid\", 4:\"subretinal fluid\", 5:\"subretinal hyperreflective material\", \n",
    "              6:\"RPE\", 7:\"fibrovascular PED\", 8:\"drusenoid PED\", 9: \"posterior hyaloid membrance\", \n",
    "              10:\"choroid\", 11:\"fibrosis\", 14:\"vitreous\", 15:\"camera effect\"}\n",
    "\n",
    "sns.set_palette(\"Greys\")\n",
    "ax = sns.barplot(x=\"class\", y=\"f1-score\", hue=\"annotator\", data=idv_pd_long)\n",
    "plt.xticks(ticks=[0,1,2,3], labels=[class_dict[5], class_dict[7], class_dict[8], class_dict[11]],\n",
    "          rotation = 90)\n",
    "\n",
    "plt.xlabel(\"\")\n",
    "\n",
    "plt.legend(title='annotator', bbox_to_anchor=(1.05, 1), loc='upper left')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "segmentation",
   "language": "python",
   "name": "segmentation"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
